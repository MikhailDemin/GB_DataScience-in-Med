{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWcUWSWpt0Rl"
   },
   "source": [
    "# Урок 4. Сверточные нейронные сети\n",
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7FBavh7yj77"
   },
   "source": [
    "### <span class=\"burk\">Задание 1.</span>\n",
    "**Сделайте краткий обзор любой архитектуры для object detection.**\n",
    "\n",
    "Проведите анализ: \n",
    " - Чем отличается выбранная вами архитектура нейронной сети от других?\n",
    " - В чём плюсы и минусы данной архитектуры?\n",
    " - Какие могут возникнуть трудности при применении этой архитектуры на практике?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpineNet** — это backbone нейросетевая архитектура для задачи распознавания объекта. Разработкой модели занимались исследователи из Google Research. Модель обходит state-of-the-art подходы на задаче распознавания объектов на данных COCO. При этом SpineNet производит на 60% меньше вычислений и обходит ResNet-FPN на 6% по AP. Предложенную архитектуру также используют для классификации. SpineNet обходит state-of-the-art на 6% в точности на датасете iNaturalist.\n",
    "\n",
    " * Обычно backbone характеризует сеть с уменьшающимся масштабом в архитектуре “энкодер-декодер”, т.е. энкодер. \n",
    " * Поскольку задача энкодера состоит в вычислении представлений признаков на основе вводных данных, backbone с уменьшающимся масштабом не сможет удерживать пространственную информацию. \n",
    " * По мере углубления слоёв признаки будут становиться всё более абстрактными и менее локализованными, тем самым усложняя извлечение декодером точных необходимых признаков. \n",
    "\n",
    "С целью преодоления сложности получения и извлечения многомасштабных признаков для локализации была представлена модель с пермутируемыми масштабами и межмасштабными связями, предлагающая следующие улучшения:\n",
    "\n",
    "1. Масштабы карт признаков в этой архитектуре получили возможность увеличения или уменьшения в любой момент времени посредством пермутации блоков, что противоположно прежнему шаблону, подразумевавшему только уменьшение. Это позволило сохранять пространственную информацию. \n",
    "2. Связям карт признаков разрешено пересекать масштабы признаков для выполнения слияния признаков из разных масштабов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Summary:_**  \n",
    "\n",
    "__Чем SpineNet отличается от предыдущих backbone-сетей?__  \n",
    "В то время как для обнаружения присутствия признака может потребоваться высокое разрешение, для определения его локализации такая же точность не требуется.  \n",
    "\n",
    "**SpineNet учится на разномасштабных изображениях за счет смешения слоев**\n",
    "\n",
    "**_Плюсы архитектуры:_**\n",
    "\n",
    "1. Использование SpineNet позволяет добиться хорошей точности при меньших затратах вычислительных ресурсов по сравнению с архитектурой ResNet\n",
    "2. При использовании SpineNet во время энкодинга теряется меньше информации с изображения\n",
    "3. Архитектура масштабируется: в [статье](https://arxiv.org/pdf/1912.05027.pdf) приводятся архитектуры SpineNet-49, SpineNet-96, SpineNet-143, SpineNet-190\n",
    "\n",
    "**_Минусы архитектуры:_**  \n",
    "1. SpineNet строится на основе архитектуры ResNet, следовательно все недостатки и ошибки ResNet присущи и SpineNet \n",
    "2. Размеры слоев подбирались с помощью нейронного поиска архитектур (NAS) что добавляет сложности при разработке модели\n",
    "\n",
    "\n",
    "**_Вероятные сложности при практической реализации:_** \n",
    "\n",
    "- В связи с использованием в SpineNet слоев переменных масштабов потребуется дополнительные затраты времени для создания пермутированных слоев которые являются переходными между слоями с разными размерами изображения и количеством признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ссылки\n",
    "\n",
    "1. Подробное объяснение архитектуры [SpineNet](https://www.youtube.com/watch?v=qFRfnIRMNlk) [EN]\n",
    "2. Статья [SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization](https://arxiv.org/pdf/1912.05027.pdf) [EN]\n",
    "3. Разбор [SpineNet: нетрадиционная архитектура](https://nuancesprog.ru/p/10090/) [RU]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY3IVQhZyj8E"
   },
   "source": [
    "### <span class=\"burk\">Задание 2$^*$.</span>\n",
    "**Ссылка на репозиторий с полным кодом для обучения ssd нейросети:\n",
    "https://github.com/sergeyveneckiy/ssd-tensorflow.** \n",
    "\n",
    "**Попробуйте улучшить точность её\n",
    "работы и напишите отчёт (что вы пробовали изменить в её параметрах и как это\n",
    "отражалось на процессе обучения нейронной сети)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-09T09:31:28.921713Z",
     "start_time": "2022-07-09T09:31:28.908772Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load 13_7_ssd-tensorflow-master/pascal-voc/download-data.sh\n",
    "#!/bin/bash\n",
    "\n",
    "wget -c http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "wget -c http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "wget -c http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "\n",
    "mkdir -p trainval\n",
    "mkdir -p test\n",
    "\n",
    "(cd trainval && tar xf ../VOCtrainval_06-Nov-2007.tar)\n",
    "(cd trainval && tar xf ../VOCtrainval_11-May-2012.tar)\n",
    "(cd test && tar xf ../VOCtest_06-Nov-2007.tar)\n",
    "\n",
    "# Download and extract data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-09T09:35:11.250516Z",
     "start_time": "2022-07-09T09:35:11.239542Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load 13_7_ssd-tensorflow-master/process_dataset.py\n",
    "#!/usr/bin/env python\n",
    "#-------------------------------------------------------------------------------\n",
    "# Author: Lukasz Janyst <lukasz@jany.st>\n",
    "# Date:   29.08.2017\n",
    "#-------------------------------------------------------------------------------\n",
    "# This file is part of SSD-TensorFlow.\n",
    "#\n",
    "# SSD-TensorFlow is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# SSD-TensorFlow is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with SSD-Tensorflow.  If not, see <http://www.gnu.org/licenses/>.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transforms import *\n",
    "from ssdutils import get_preset_by_name\n",
    "from utils import load_data_source, str2bool, draw_box\n",
    "from tqdm import tqdm\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"This is a Python 3 program. Use Python 3 or higher.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def annotate(data_dir, samples, colors, sample_name):\n",
    "    \"\"\"\n",
    "    Draw the bounding boxes on the sample images\n",
    "    :param data_dir: the directory where the dataset's files are stored\n",
    "    :param samples:  samples to be processed\n",
    "    :param colors:   a dictionary mapping class name to a BGR color tuple\n",
    "    :param colors:   name of the sample\n",
    "    \"\"\"\n",
    "    result_dir = data_dir+'/annotated/'+sample_name.strip()+'/'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    for sample in tqdm(samples, desc=sample_name, unit='samples'):\n",
    "        img    = cv2.imread(sample.filename)\n",
    "        basefn = os.path.basename(sample.filename)\n",
    "        for box in sample.boxes:\n",
    "            draw_box(img, box, colors[box.label])\n",
    "        cv2.imwrite(result_dir+basefn, img)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def build_sampler(overlap, trials):\n",
    "    return SamplerTransform(sample=True, min_scale=0.3, max_scale=1.0,\n",
    "                            min_aspect_ratio=0.5, max_aspect_ratio=2.0,\n",
    "                            min_jaccard_overlap=overlap, max_trials=trials)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def build_train_transforms(preset, num_classes, sampler_trials, expand_prob):\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Resizing\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_resize = ResizeTransform(width=preset.image_size.w,\n",
    "                                height=preset.image_size.h,\n",
    "                                algorithms=[cv2.INTER_LINEAR,\n",
    "                                            cv2.INTER_AREA,\n",
    "                                            cv2.INTER_NEAREST,\n",
    "                                            cv2.INTER_CUBIC,\n",
    "                                            cv2.INTER_LANCZOS4])\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Image distortions\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_brightness = BrightnessTransform(delta=32)\n",
    "    tf_rnd_brightness = RandomTransform(prob=0.5, transform=tf_brightness)\n",
    "\n",
    "    tf_contrast = ContrastTransform(lower=0.5, upper=1.5)\n",
    "    tf_rnd_contrast = RandomTransform(prob=0.5, transform=tf_contrast)\n",
    "\n",
    "    tf_hue = HueTransform(delta=18)\n",
    "    tf_rnd_hue = RandomTransform(prob=0.5, transform=tf_hue)\n",
    "\n",
    "    tf_saturation = SaturationTransform(lower=0.5, upper=1.5)\n",
    "    tf_rnd_saturation = RandomTransform(prob=0.5, transform=tf_saturation)\n",
    "\n",
    "    tf_reorder_channels = ReorderChannelsTransform()\n",
    "    tf_rnd_reorder_channels = RandomTransform(prob=0.5,\n",
    "                                              transform=tf_reorder_channels)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Compositions of image distortions\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_distort_lst = [\n",
    "        tf_rnd_contrast,\n",
    "        tf_rnd_saturation,\n",
    "        tf_rnd_hue,\n",
    "        tf_rnd_contrast\n",
    "    ]\n",
    "    tf_distort_1 = ComposeTransform(transforms=tf_distort_lst[:-1])\n",
    "    tf_distort_2 = ComposeTransform(transforms=tf_distort_lst[1:])\n",
    "    tf_distort_comp = [tf_distort_1, tf_distort_2]\n",
    "    tf_distort = TransformPickerTransform(transforms=tf_distort_comp)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Expand sample\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_expand = ExpandTransform(max_ratio=4.0, mean_value=[104, 117, 123])\n",
    "    tf_rnd_expand = RandomTransform(prob=expand_prob, transform=tf_expand)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Samplers\n",
    "    #---------------------------------------------------------------------------\n",
    "    samplers = [\n",
    "        SamplerTransform(sample=False),\n",
    "        build_sampler(0.1, sampler_trials),\n",
    "        build_sampler(0.3, sampler_trials),\n",
    "        build_sampler(0.5, sampler_trials),\n",
    "        build_sampler(0.7, sampler_trials),\n",
    "        build_sampler(0.9, sampler_trials),\n",
    "        build_sampler(1.0, sampler_trials)\n",
    "    ]\n",
    "    tf_sample_picker = SamplePickerTransform(samplers=samplers)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Horizontal flip\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_flip = HorizontalFlipTransform()\n",
    "    tf_rnd_flip = RandomTransform(prob=0.5, transform=tf_flip)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Transform list\n",
    "    #---------------------------------------------------------------------------\n",
    "    transforms = [\n",
    "        ImageLoaderTransform(),\n",
    "        tf_rnd_brightness,\n",
    "        tf_distort,\n",
    "        tf_rnd_reorder_channels,\n",
    "        tf_rnd_expand,\n",
    "        tf_sample_picker,\n",
    "        tf_rnd_flip,\n",
    "        LabelCreatorTransform(preset=preset, num_classes=num_classes),\n",
    "        tf_resize\n",
    "    ]\n",
    "    return transforms\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def build_valid_transforms(preset, num_classes):\n",
    "    tf_resize = ResizeTransform(width=preset.image_size.w,\n",
    "                                height=preset.image_size.h,\n",
    "                                algorithms=[cv2.INTER_LINEAR])\n",
    "    transforms = [\n",
    "        ImageLoaderTransform(),\n",
    "        LabelCreatorTransform(preset=preset, num_classes=num_classes),\n",
    "        tf_resize\n",
    "    ]\n",
    "    return transforms\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def main():\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Parse the commandline\n",
    "    #---------------------------------------------------------------------------\n",
    "    parser = argparse.ArgumentParser(description='Process a dataset for SSD')\n",
    "    parser.add_argument('--data-source', default='pascal_voc',\n",
    "                        help='data source')\n",
    "    parser.add_argument('--data-dir', default='pascal-voc',\n",
    "                        help='data directory')\n",
    "    parser.add_argument('--validation-fraction', type=float, default=0.025,\n",
    "                        help='fraction of the data to be used for validation')\n",
    "    parser.add_argument('--expand-probability', type=float, default=0.5,\n",
    "                        help='probability of running sample expander')\n",
    "    parser.add_argument('--sampler-trials', type=int, default=50,\n",
    "                        help='number of time a sampler tries to find a sample')\n",
    "    parser.add_argument('--annotate', type=str2bool, default='False',\n",
    "                        help=\"Annotate the data samples\")\n",
    "    parser.add_argument('--compute-td', type=str2bool, default='True',\n",
    "                        help=\"Compute training data\")\n",
    "    parser.add_argument('--preset', default='vgg300',\n",
    "                        choices=['vgg300', 'vgg512'],\n",
    "                        help=\"The neural network preset\")\n",
    "    parser.add_argument('--process-test', type=str2bool, default='False',\n",
    "                        help=\"process the test dataset\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('[i] Data source:          ', args.data_source)\n",
    "    print('[i] Data directory:       ', args.data_dir)\n",
    "    print('[i] Validation fraction:  ', args.validation_fraction)\n",
    "    print('[i] Expand probability:   ', args.expand_probability)\n",
    "    print('[i] Sampler trials:       ', args.sampler_trials)\n",
    "    print('[i] Annotate:             ', args.annotate)\n",
    "    print('[i] Compute training data:', args.compute_td)\n",
    "    print('[i] Preset:               ', args.preset)\n",
    "    print('[i] Process test dataset: ', args.process_test)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Load the data source\n",
    "    #---------------------------------------------------------------------------\n",
    "    print('[i] Configuring the data source...')\n",
    "    try:\n",
    "        source = load_data_source(args.data_source)\n",
    "        source.load_trainval_data(args.data_dir, args.validation_fraction)\n",
    "        if args.process_test:\n",
    "            source.load_test_data(args.data_dir)\n",
    "        print('[i] # training samples:   ', source.num_train)\n",
    "        print('[i] # validation samples: ', source.num_valid)\n",
    "        print('[i] # testing samples:    ', source.num_test)\n",
    "        print('[i] # classes:            ', source.num_classes)\n",
    "    except (ImportError, AttributeError, RuntimeError) as e:\n",
    "        print('[!] Unable to load data source:', str(e))\n",
    "        return 1\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Annotate samples\n",
    "    #---------------------------------------------------------------------------\n",
    "    if args.annotate:\n",
    "        print('[i] Annotating samples...')\n",
    "        annotate(args.data_dir, source.train_samples, source.colors, 'train')\n",
    "        annotate(args.data_dir, source.valid_samples, source.colors, 'valid')\n",
    "        if args.process_test:\n",
    "            annotate(args.data_dir, source.test_samples,  source.colors, 'test ')\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Compute the training data\n",
    "    #---------------------------------------------------------------------------\n",
    "    if args.compute_td:\n",
    "        preset = get_preset_by_name(args.preset)\n",
    "        with open(args.data_dir+'/train-samples.pkl', 'wb') as f:\n",
    "            pickle.dump(source.train_samples, f)\n",
    "        with open(args.data_dir+'/valid-samples.pkl', 'wb') as f:\n",
    "            pickle.dump(source.valid_samples, f)\n",
    "\n",
    "        with open(args.data_dir+'/training-data.pkl', 'wb') as f:\n",
    "            data = {\n",
    "                'preset': preset,\n",
    "                'num-classes': source.num_classes,\n",
    "                'colors': source.colors,\n",
    "                'lid2name': source.lid2name,\n",
    "                'lname2id': source.lname2id,\n",
    "                'train-transforms': build_train_transforms(preset,\n",
    "                                       source.num_classes, args.sampler_trials,\n",
    "                                       args.expand_probability ),\n",
    "                'valid-transforms': build_valid_transforms(preset,\n",
    "                                                           source.num_classes)\n",
    "            }\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-09T09:58:38.563325Z",
     "start_time": "2022-07-09T09:58:38.513134Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'escape' from 'cgi' (C:\\Users\\demin\\MySoft\\Anaconda_05_2022\\lib\\cgi.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mssdutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_preset_by_name\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_source, str2bool, draw_box\n",
      "File \u001b[1;32m~\\MySoft\\Anaconda_05_2022\\lib\\site-packages\\transforms\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_html\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_html, bodyfinder\n",
      "File \u001b[1;32m~\\MySoft\\Anaconda_05_2022\\lib\\site-packages\\transforms\\safe_html.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msgmllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGMLParser, SGMLParseError\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safeToInt\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_html_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'escape' from 'cgi' (C:\\Users\\demin\\MySoft\\Anaconda_05_2022\\lib\\cgi.py)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transforms import *\n",
    "from ssdutils import get_preset_by_name\n",
    "from utils import load_data_source, str2bool, draw_box\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyVqyez7Z6zV"
   },
   "source": [
    "### <span class=\"girk\">Выводы:</span>  \n",
    "\n",
    "**Интересный проект. К сожалению не смог развернуть его из репозитория -  не хватает навыков. Хотя скачал файлы и датасет. Кроме того не смог найти нужные библиотеки использованные в проекте**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsKhUR8fywAm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13_lesson_4_my_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
